# ST0263 - T√ìPICOS ESPECIALES DE TELEM√ÅTICA
## Estudiantes:
## Luis Fernando Posada Cano - lfposadac@eafit.edu.co
## Jose Miguel Burgos Cuartas - jmburgoc@eafit.edu.co
## Marcela Londo√±o Leon - mlondonol@eafit.edu.co
## Juanmartin Betancur Arango - jbetancur5@eafit.edu.co
## Proyecto3: Automatizaci√≥n de un proceso batch de datos big data en azure

- - -
## 1. Breve descripci√≥n de la actividad

Este proyecto consiste en la automatizaci√≥n completa de un flujo de procesamiento de datos tipo batch usando tecnolog√≠as de la nube. Se trabaj√≥ con datos de comercio electr√≥nico simulados provenientes de la Fake Store API y una base de datos relacional MySQL alojada en una m√°quina virtual (VM) en Azure.

El pipeline incluye la captura de datos, su ingesta en almacenamiento tipo data lake, procesamiento ETL y an√°lisis de datos descriptivos y avanzados, utilizando herramientas como **Azure Blob Storage**, **Azure Databricks** y **Python**. La ejecuci√≥n del flujo completo se automatiz√≥ mediante un **Job en Databricks**.

### 1.1 Aspectos cumplidos de la actividad propuesta por el profesor

- Uso de dos fuentes de datos reales: una API p√∫blica (Fake Store API) y una base de datos relacional simulada (MySQL en VM).
- Ingesta autom√°tica a contenedores tipo "raw" en Blob Storage.
- Procesamiento autom√°tico con Spark (ETL y an√°lisis) sobre Databricks.
- Almacenamiento de resultados en zona ‚Äúrefined‚Äù del data lake.
- Automatizaci√≥n del proceso con Jobs en Databricks.
- Video de sustentaci√≥n mostrando el funcionamiento del pipeline automatizado.

### 1.2 Aspectos no cumplidos de la actividad propuesta por el profesor

- No se us√≥ Athena para consulta de datos, ya que la implementaci√≥n fue en Azure.

---

## 2. Informaci√≥n general de dise√±o de alto nivel, arquitectura, patrones, mejores pr√°cticas utilizadas

- **Arquitectura por zonas:** datos se almacenan en contenedores `raw`, `trusted`, y `refined` en Azure Blob Storage.
- **Separaci√≥n por scripts modulares** para cada etapa: captura, ingesta, procesamiento, an√°lisis.
- **Automatizaci√≥n con Azure Databricks Jobs**: el flujo completo corre autom√°ticamente sin intervenci√≥n manual.
- **Persistencia estructurada y formatos est√°ndar** (`JSON`, `Parquet`) para almacenamiento intermedio y final.
- **Notebooks en Databricks** para facilitar pruebas, desarrollo y seguimiento del procesamiento.

---

## 3. Descripci√≥n del ambiente de desarrollo y t√©cnico
### 3.1 C√≥mo se compila y ejecuta

1. **Captura de datos desde la API**
   - Script: `capture_data.py`
   - Descarga datos de `/products`, `/users` y `/carts` desde https://fakestoreapi.com y los guarda en archivos `.json`.

2. **Ingesta a base de datos relacional simulada**
   - Script: `insert_data_mysql.py`
   - Carga los archivos `.json` a una base de datos MySQL ejecut√°ndose en una VM de Azure.

3. **Extracci√≥n desde la base de datos a Azure Blob Storage**
   - Script: `mysql_to_blobstorage.py`
   - Extrae los datos desde la base MySQL y los sube a la zona `raw/relational/` de Blob Storage.

4. **Procesamiento ETL**
   - Script: `etl.py` (Notebook en Databricks)
   - Limpia y une datos de la API con la base relacional y los guarda en la zona `trusted`.

5. **An√°lisis de datos**
   - Script: `analysis.py` (Notebook en Databricks)
   - Realiza an√°lisis descriptivo sobre los datos procesados.
   - Resultados almacenados en `refined`.

6. **Automatizaci√≥n**
   - Se cre√≥ un **Job en Databricks** que ejecuta en secuencia los notebooks de ETL y an√°lisis.

### 3.2 Detalles del desarrollo

- Lenguaje principal: **Python 3**
- Desarrollo distribuido entre scripts locales y notebooks en Databricks.
- Organizaci√≥n modular: cada etapa tiene su propio script/notebook independiente.

### 3.3 Detalles t√©cnicos

- **Azure Blob Storage:**
  - `raw/` ‚Üí Datos crudos desde API y MySQL
  - `trusted/` ‚Üí Datos procesados
  - `refined/` ‚Üí Resultados del an√°lisis

- **Base de datos relacional:**
  - MySQL 8.0 corriendo en una VM Ubuntu en Azure.
  - Tablas simuladas de usuarios, carritos, y productos.

- **Cluster de procesamiento:**
  - Azure Databricks (cl√∫ster autoservicio)

### 3.4 Descripci√≥n y configuraci√≥n de par√°metros del proyecto

- Variables de entorno requeridas:
  - `AZURE_STORAGE_CONNECTION_STRING`
  - Credenciales de conexi√≥n a MySQL (host, puerto, usuario, contrase√±a)
- Configuraci√≥n del Job en Databricks:
  - Secuencia de tareas:
    1. `etl.py`
    2. `analysis.py`
  - Programado para ejecuci√≥n manual o peri√≥dica.

---

## 4. Video de sustentaci√≥n

üé• Video disponible en:  
[https://www.youtube.com/watch?v=gv8I-KAUOxU&feature=youtu.be](https://www.youtube.com/watch?v=gv8I-KAUOxU&feature=youtu.be)

## 5. Otra informaci√≥n que considere relevante para esta actividad

Durante el desarrollo de este proyecto se exploraron herramientas modernas para procesamiento batch de datos en la nube. Se destaca lo siguiente:

La automatizaci√≥n con Databricks Jobs simplific√≥ la orquestaci√≥n sin necesidad de herramientas adicionales como Airflow.

El uso de Azure Blob Storage como Data Lake permiti√≥ mantener una arquitectura limpia separando las zonas de datos (raw, trusted, refined).

La simulaci√≥n de una base de datos relacional en una VM fue √∫til para replicar escenarios reales de integraci√≥n de m√∫ltiples fuentes.

La elecci√≥n de la Fake Store API result√≥ apropiada para simular un entorno de ecommerce, y permiti√≥ integrar datos con estructura variada (productos, carritos, usuarios).

üîó Referencias y cr√©ditos
Fake Store API ‚Äì https://fakestoreapi.com/

Video de referencia para EMR con Spark (como gu√≠a inicial del enfoque):
https://www.youtube.com/watch?v=ZFns7fvBCH4

Documentaci√≥n oficial de:

Azure Databricks: https://learn.microsoft.com/en-us/azure/databricks/

Azure Blob Storage: https://learn.microsoft.com/en-us/azure/storage/blobs/

Curso ST0263 - Recursos compartidos por el profesor

Stack Overflow ‚Äì Soluciones a errores con conexi√≥n a MySQL y scripts en Python

---
Video: https://youtu.be/gv8I-KAUOxU
